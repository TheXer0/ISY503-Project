{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12551fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project files stored in git repository. Below is the link to the repository.\n",
    "#https://github.com/TheXer0/ISY503-Project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e6184c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\xer0\\anaconda3\\lib\\site-packages (6.0.4)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in c:\\users\\xer0\\anaconda3\\lib\\site-packages (from inflect) (1.10.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\xer0\\anaconda3\\lib\\site-packages (from pydantic>=1.9.1->inflect) (4.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\xer0\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\xer0\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\xer0\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\xer0\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\xer0\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\xer0\\anaconda3\\lib\\site-packages)\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Xer0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Xer0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Xer0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Xer0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Xer0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "\n",
    "!pip3 install inflect\n",
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import gensim\n",
    "import string\n",
    "import inflect\n",
    "import torch.nn.functional as F\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "p = inflect.engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce5ffdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes non-English words from a string of text\n",
    "#splits text into individual words, checks if each word is in a set of English words, \n",
    "#and then joins the remaining English words back together into a string\n",
    "english_words = set(words.words())\n",
    "def remove_non_english_words(text):\n",
    "    words_list = text.split()\n",
    "    filtered_words = [word for word in words_list if word.lower() in english_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19ce391b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1828\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>5.0</td>\n",
       "      <td>this book was a gift given to me by a friend a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>5.0</td>\n",
       "      <td>you will either kill them or become a video on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>4.0</td>\n",
       "      <td>we would expect a well balanced approach to wr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>4.0</td>\n",
       "      <td>or so my wife me she half the frozen section f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1.0</td>\n",
       "      <td>why are there so many insanely positive for th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                        review_text  result\n",
       "1362    5.0   this book was a gift given to me by a friend a...       1\n",
       "1340    5.0   you will either kill them or become a video on...       1\n",
       "972     4.0   we would expect a well balanced approach to wr...       1\n",
       "1682    4.0   or so my wife me she half the frozen section f...       1\n",
       "468     1.0   why are there so many insanely positive for th...       0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleans and filters a text input by removing punctuation and non-English words\n",
    "\n",
    "def cleanReview(text):\n",
    "  txt = text.translate(str.maketrans('','',string.punctuation))\n",
    "  txt = remove_non_english_words(txt)\n",
    "  return txt\n",
    "\n",
    "def readFiles(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        texto = f.read()  \n",
    "    # Convert all text to lowercase\n",
    "    texto = texto.lower()\n",
    "    # Replace newline characters with empty string\n",
    "    texto = texto.replace('\\n', '')\n",
    "    # Find all tags in the text and remove closing tags\n",
    "    textohelp = re.sub(r'</.*?>', '', texto)\n",
    "    # Extract names of all tags in the text\n",
    "    etiquetas = re.findall(r'<.*?>', textohelp)\n",
    "    nombres_etiquetas = [re.sub(r'[<>]', '', etiqueta) for etiqueta in etiquetas]\n",
    "    # Find unique tags\n",
    "    etiquetas_unicas = set(nombres_etiquetas)\n",
    "    # Create a BeautifulSoup object from the text\n",
    "    soup = BeautifulSoup(texto, 'html.parser')\n",
    "    # Find all review elements and create a Python object\n",
    "    reviews =soup.find_all('review')\n",
    "    return  reviews\n",
    "\n",
    "def loadReviews(revs,reviewType):\n",
    "    #empty list\n",
    "  reviewList = []\n",
    "  for a in revs:\n",
    "        #empty dictionaries dict\n",
    "        dict = {}\n",
    "        # Replace newline characters with empty string\n",
    "        textoaux = a.prettify().replace('\\n', '')\n",
    "        if len(textoaux)>3000 or len(textoaux)<10 :\n",
    "            continue\n",
    "        try:\n",
    "             # code that might raise an error if read a unknown character\n",
    "            raiz = ET.fromstring(textoaux)\n",
    "        except:\n",
    "            continue\n",
    "        # Mapping the new object\n",
    "        if raiz.find('rating') is not None:\n",
    "            dict['rating'] = raiz.find('rating').text\n",
    "        if raiz.find('review_text') is not None:\n",
    "            dict['review_text'] = cleanReview(raiz.find('review_text').text)\n",
    "        dict['result']=reviewType\n",
    "        \n",
    "        reviewList.append(dict)\n",
    "  return reviewList\n",
    "\n",
    "\n",
    "# result = 0 NEGATIVE\n",
    "raw_dataseta = readFiles('data/negative.review')\n",
    "negRevs = loadReviews(raw_dataseta,0)\n",
    "\n",
    "# result = 1 POSITIVE\n",
    "raw_datasetb =  readFiles('data/positive.review')\n",
    "posRevs = loadReviews(raw_datasetb,1)\n",
    "\n",
    "# Full dataset NEG & POS reviews\n",
    "data = negRevs + posRevs \n",
    "df = pd.DataFrame(data)\n",
    "dataset = df.dropna()\n",
    "dataset = dataset.copy()\n",
    "\n",
    "print(len(dataset))\n",
    "# Randomize the dataset\n",
    "dataset = dataset.reindex(np.random.permutation(dataset.index))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f23a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove repeating words\n",
    "#takes in a list of sentences as input, \n",
    "#and uses the gensim library to preprocess each sentence by removing repeating words.\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "accffb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'book', 'was', 'gift', 'given', 'to', 'me', 'by', 'friend', 'ago', 'own', 'and', 'have', 'read', 'about', 'twelve', 'on', 'the', 'topic', 'out', 'of', 'all', 'of', 'them', 'this', 'been', 'by', 'far', 'the', 'best', 'by', 'the', 'end', 'it', 'like', 'journey', 'inside', 'yourself', 'taken', 'place', 'and', 'you', 'arrive', 'to', 'new', 'destination', 'on', 'your', 'the', 'feel', 'of', 'the', 'book', 'is', 'one', 'of', 'love', 'and', 'support', 'not', 'the', 'type', 'or', 'author', 'as', 'guru', 'type', 'that', 'have', 'read', 'in', 'other', 'one', 'the', 'experience', 'of', 'two', 'professional', 'that', 'itself', 'in', 'each', 'chapter', 'it', 'made', 'large', 'difference', 'in', 'my', 'struggle', 'with', 'an', 'eating', 'disorder', 'that', 'at', 'age', 'am', 'now'], ['you', 'will', 'either', 'kill', 'them', 'or', 'become', 'video', 'on', 'television', 'that', 'for', 'modern', 'desensitization'], ['we', 'would', 'expect', 'well', 'balanced', 'approach', 'to', 'writing', 'about', 'the', 'work', 'of', 'the', 'holy', 'spirit', 'in', 'the', 'church', 'by', 'the', 'author', 'who', 'also', 'wrote', 'tyranny', 'of', 'the', 'urgent', 'and', 'we', 'get', 'it', 'in', 'this', 'book', 'hummel', 'former', 'president', 'of', 'fellowship', 'first', 'the', 'history', 'and', 'growth', 'of', 'the', 'various', 'charismatic', 'within', 'both', 'the', 'protestant', 'and', 'catholic', 'he', 'many', 'valuable', 'of', 'the', 'meaning', 'of', 'related', 'as', 'well', 'as', 'of', 'and', 'that', 'he', 'himself', 'seen', 'he', 'later', 'his', 'for', 'keeping', 'the', 'holy', 'spirit', 'fire', 'in', 'the', 'fireplace', 'of', 'church', 'body', 'came', 'away', 'in', 'my', 'own', 'spiritual', 'and', 'that', 'the', 'more', 'extraordinary', 'spirit', 'have', 'not', 'and', 'that', 'to', 'each', 'one', 'is', 'given', 'the', 'manifestation', 'of', 'the', 'spirit', 'for', 'the', 'common', 'good'], ['or', 'so', 'my', 'wife', 'me', 'she', 'half', 'the', 'frozen', 'section', 'from', 'the', 'library', 'before', 'we', 'should', 'buy', 'the', 'book', 'so', 'far', 'as', 'can', 'remember', 'they', 'were', 'all', 'good'], ['why', 'are', 'there', 'so', 'many', 'insanely', 'positive', 'for', 'this', 'book', 'honestly', 'what', 'this', 'author', 'to', 'the', 'world', 'with', 'this', 'book', 'was', 'it', 'the', 'no', 'all', 'been', 'used', 'and', 'and', 'used', 'some', 'more', 'in', 'practically', 'every', 'other', 'work', 'of', 'fiction', 'in', 'the', 'last', 'fifty', 'ie', 'cocky', 'and', 'handsome', 'agent', 'beautiful', 'but', 'dangerous', 'killer', 'with', 'past', 'boss', 'was', 'it', 'the', 'deep', 'insight', 'into', 'the', 'themselves', 'laugh', 'out', 'loud', 'no', 'there', 'was', 'nothing', 'coming', 'close', 'to', 'character', 'development', 'was', 'there', 'some', 'new', 'and', 'clever', 'writing', 'style', 'unless', 'you', 'count', 'two', 'page', 'revolutionary', 'call', 'it', 'way', 'to', 'have', 'two', 'hundred', 'page', 'novella', 'turned', 'into', 'four', 'hundred', 'page', 'novel', 'how', 'about', 'the', 'plot', 'was', 'that', 'something', 'worth', 'my', 'time', 'not', 'unless', 'you', 'havent', 'ever', 'watched', 'movie', 'in', 'your', 'life', 'so', 'honestly', 'what', 'the', 'hell', 'this', 'book', 'worth', 'time', 'all', 'can', 'find', 'in', 'this', 'book', 'are', 'two', 'dimensional', 'needless', 'dialogue', 'and', 'empty', 'were', 'lied', 'to', 'about', 'the', 'main', 'fate', 'in', 'the', 'beginning', 'and', 'were', 'lied', 'to', 'about', 'this', 'being', 'thriller', 'in', 'the', 'vein', 'of', 'as', 'person', 'who', 'turned', 'to', 'the', 'world', 'of', 'to', 'escape', 'onslaught', 'of', 'needless', 'pointless', 'and', 'time', 'wasting', 'strategy', 'of', 'churning', 'out', 'movie', 'after', 'movie', 'of', 'pure', 'garbage', 'can', 'say', 'that', 'this', 'is', 'the', 'type', 'of', 'book', 'that', 'will', 'doom', 'us', 'all', 'into', 'market', 'of', 'hurried', 'and', 'horribly', 'bad', 'to', 'take', 'our', 'hard', 'money', 'this', 'man', 'does', 'not', 'deserve', 'our', 'money', 'and', 'certainly', 'does', 'not', 'deserve', 'to', 'write', 'anything', 'other', 'than', 'an', 'apology', 'letter', 'to', 'the', 'world', 'for', 'us', 'down', 'into', 'thinking', 'that', 'this', 'type', 'of', 'writing', 'is', 'anything', 'other', 'than', 'trash']]\n",
      "['this book was gift given to me by friend ago own and have read about twelve on the topic out of all of them this been by far the best by the end it like journey inside yourself taken place and you arrive to new destination on your the feel of the book is one of love and support not the type or author as guru type that have read in other one the experience of two professional that itself in each chapter it made large difference in my struggle with an eating disorder that at age am now', 'you will either kill them or become video on television that for modern desensitization', 'we would expect well balanced approach to writing about the work of the holy spirit in the church by the author who also wrote tyranny of the urgent and we get it in this book hummel former president of fellowship first the history and growth of the various charismatic within both the protestant and catholic he many valuable of the meaning of related as well as of and that he himself seen he later his for keeping the holy spirit fire in the fireplace of church body came away in my own spiritual and that the more extraordinary spirit have not and that to each one is given the manifestation of the spirit for the common good', 'or so my wife me she half the frozen section from the library before we should buy the book so far as can remember they were all good', 'why are there so many insanely positive for this book honestly what this author to the world with this book was it the no all been used and and used some more in practically every other work of fiction in the last fifty ie cocky and handsome agent beautiful but dangerous killer with past boss was it the deep insight into the themselves laugh out loud no there was nothing coming close to character development was there some new and clever writing style unless you count two page revolutionary call it way to have two hundred page novella turned into four hundred page novel how about the plot was that something worth my time not unless you havent ever watched movie in your life so honestly what the hell this book worth time all can find in this book are two dimensional needless dialogue and empty were lied to about the main fate in the beginning and were lied to about this being thriller in the vein of as person who turned to the world of to escape onslaught of needless pointless and time wasting strategy of churning out movie after movie of pure garbage can say that this is the type of book that will doom us all into market of hurried and horribly bad to take our hard money this man does not deserve our money and certainly does not deserve to write anything other than an apology letter to the world for us down into thinking that this type of writing is anything other than trash']\n"
     ]
    }
   ],
   "source": [
    "#converts a list of words back into a sentence\n",
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)\n",
    "\n",
    "#preprocesses text by splitting it into words, detokenizing the words, \n",
    "#and creating a list of preprocessed documents called data from a Pandas DataFrame column called review_text.\n",
    "\n",
    "temp = []\n",
    "#Splitting pd.Series to list\n",
    "data_to_list = dataset['review_text'].values.tolist()\n",
    "for i in range(len(data_to_list)):\n",
    "    temp.append((data_to_list[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "print(data_words[:5])\n",
    "data = []\n",
    "for i in range(len(data_words)):\n",
    "    data.append(detokenize(data_words[i]))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f1a778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the data and delete stop words\n",
    "#returns a list of cleaned and lemmatized reviews\n",
    "\n",
    "def text_procesing(text):\n",
    "  STOPWORDS = set(stopwords.words('english'))\n",
    "  cleaned_reviews = []\n",
    "  for review in text:\n",
    "    tokens = [word for word in word_tokenize(review) if not word in STOPWORDS]\n",
    "    cleaned_reviews.append(\" \".join(tokens))\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lem_reviews = []\n",
    "  for review in cleaned_reviews:\n",
    "    lem_reviews.append(\" \".join(list(map(lemmatizer.lemmatize, word_tokenize(review)))))\n",
    "  return lem_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed062517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  671 2109  284]\n",
      " [   0    0    0 ... 1012 3296  272]\n",
      " [   0    0    0 ... 1151  465    7]\n",
      " ...\n",
      " [   0    0    0 ...  224 3516 4845]\n",
      " [   0    0    0 ...   89   90  619]\n",
      " [   0    0    0 ... 1190  522    1]]\n"
     ]
    }
   ],
   "source": [
    "y = dataset['review_text']\n",
    "dataset['review_text'] =text_procesing(y)\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 200\n",
    "# Create the token array and their sequences\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(dataset['review_text'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['review_text'])\n",
    "reviews_sequences = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "print(reviews_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd3ebc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Y input variable sentiment categories \n",
    "result_sentiment = keras.utils.to_categorical(dataset['result'].astype('float32'), 2, dtype=\"float32\")\n",
    "print(result_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e30c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371 457 1371 457\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data\n",
    "#The reviews_sequences and result_sentiment are the X and y input variables, respectively\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_sequences,result_sentiment, random_state=0)\n",
    "print (len(X_train),len(X_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb9f34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the NN Sequential model using Keras Sequential API\n",
    "\n",
    "fmodel = Sequential()\n",
    "embedding_layer = Embedding(1000, 64)\n",
    "fmodel.add(layers.Embedding(max_words, 15))\n",
    "fmodel.add(layers.SimpleRNN(15,return_sequences=True))\n",
    "fmodel.add(layers.SimpleRNN(15))\n",
    "fmodel.add(layers.Dense(2,activation='softmax'))\n",
    "\n",
    "#creates a sequential neural network model named 'model' and adds layers to it\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(1000, 64)\n",
    "model.add(layers.Embedding(max_words, 15))\n",
    "model.add(layers.SimpleRNN(15,return_sequences=True))\n",
    "model.add(layers.SimpleRNN(15))\n",
    "model.add(layers.Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90c6d786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 3s 48ms/step - loss: 0.7004 - accuracy: 0.5004 - val_loss: 0.6927 - val_accuracy: 0.5186\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.5785 - accuracy: 0.7520 - val_loss: 0.7126 - val_accuracy: 0.5295\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 2s 41ms/step - loss: 0.3441 - accuracy: 0.9015 - val_loss: 0.7779 - val_accuracy: 0.5164\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.1696 - accuracy: 0.9657 - val_loss: 0.8831 - val_accuracy: 0.5230\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 2s 41ms/step - loss: 0.0818 - accuracy: 0.9825 - val_loss: 0.9713 - val_accuracy: 0.5033\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 2s 41ms/step - loss: 0.0401 - accuracy: 0.9934 - val_loss: 1.1176 - val_accuracy: 0.5383\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 2s 40ms/step - loss: 0.0193 - accuracy: 0.9985 - val_loss: 1.2361 - val_accuracy: 0.5164\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 2s 41ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 1.3298 - val_accuracy: 0.4989\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 2s 40ms/step - loss: 0.0054 - accuracy: 0.9993 - val_loss: 1.4764 - val_accuracy: 0.5098\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 2s 41ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 1.5693 - val_accuracy: 0.5055\n"
     ]
    }
   ],
   "source": [
    "#compiles a model using the \"rmsprop\" optimizer and \"categorical_crossentropy\" loss function\n",
    "#trains the model for 10 epochs\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# history = fmodel.fit(X_train, y_train, epochs=10,callbacks=[PrintDot()])\n",
    "history = model.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b33c7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 3s 37ms/step - loss: 0.7055 - accuracy: 0.5237\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 0.5318 - accuracy: 0.7338\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 2s 36ms/step - loss: 0.2986 - accuracy: 0.8862\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 0.1483 - accuracy: 0.9504\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 0.0663 - accuracy: 0.9781\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 0.0361 - accuracy: 0.9898\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 2s 38ms/step - loss: 0.0147 - accuracy: 0.9964\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 0.0114 - accuracy: 0.9964\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 2s 36ms/step - loss: 0.0044 - accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 0.0039 - accuracy: 0.9985\n"
     ]
    }
   ],
   "source": [
    "#fits the model to the training data for 10 epochs\n",
    "fmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = fmodel.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "052be7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set with validation data \n",
    "#fmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#history = fmodel.fit(X_train, y_train, epochs=10,callbacks=[PrintDot()], validation_data=(X_test, y_test))\n",
    "\n",
    "# training set with 20% of the training data as validation data\n",
    "#fmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#history = fmodel.fit(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b7fa17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a list of two strings, \"Negative\" and \"Positive\", \n",
    "#representing the two sentiment categories in the dataset\n",
    "\n",
    "sentiment = ['Negative','Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76563f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRITE THE SENTENCE AS AN INPUT HERE:The product makes good smothies and also is great for juices\n"
     ]
    }
   ],
   "source": [
    "#prompts the user to input a review\n",
    "review_input = input(\"WRITE THE SENTENCE AS AN INPUT HERE:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ab84326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_input= 'the product is horrible, and useless'\n",
    "# review_input= 'this experience has been the worst , want all my money back'\n",
    "# review_input='I never even traveled with it. It just stopped working completely making me lose data. NEVER AGAIN I'll buy anything from this company. This drive is a piece of rubbish!'\n",
    "\n",
    "# POSITIVE Test reviews\n",
    "# review_input = 'The product makes good smothies and also is great for juices'\n",
    "# review_input= 'definitely recommend this memory card, download time is great'\n",
    "# review_input= 'I borrowed this book on CD from the library and half way through, ordered the book so I would have it to refer to. Excellent book, highly recommended. Helps to put a realistic perspective on millionaires'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2edc9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses the tokenizer to convert it to a sequence of tokens\n",
    "tokensPrediction= tokenizer.texts_to_sequences([review_input])\n",
    "sequencePrediction= sequence.pad_sequences(tokensPrediction, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d42ff519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a prediction of the sentiment based on a pre-trained model\n",
    "# MODEL WITHOUT VALIDATION DATA\n",
    "\n",
    "#test= sequence.pad_sequences(sequencer, maxlen=max_len)\n",
    "#predictions =fmodel.predict(test) \n",
    "#print(predictions)\n",
    "#sentiment[np.around(predictions, decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6556628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 221ms/step\n",
      "[[0.08114872 0.9188513 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##generates a prediction of the sentiment based on trained RNN model\n",
    "# MODEL WITH VALIDATION DATA\n",
    "predictions =model.predict(sequencePrediction) \n",
    "print(predictions)\n",
    "sentiment[np.around(predictions, decimals=0).argmax(axis=1)[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
